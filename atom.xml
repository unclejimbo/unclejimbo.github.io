<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>unclejimbo&#39;s site</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://unclejimbo.github.io/"/>
  <updated>2017-04-03T19:54:44.524Z</updated>
  <id>https://unclejimbo.github.io/</id>
  
  <author>
    <name>unclejimbo (王少东)</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Looping Performance in C++</title>
    <link href="https://unclejimbo.github.io/2017/04/03/Looping-Performance-in-C++/"/>
    <id>https://unclejimbo.github.io/2017/04/03/Looping-Performance-in-C++/</id>
    <published>2017-04-03T19:21:00.000Z</published>
    <updated>2017-04-03T19:54:44.524Z</updated>
    
    <content type="html"><![CDATA[<p>Today I was testing the performance of a piece of code, which is basically accessing each element in a container within a for loop. But the result is quite shocking because I found the std::for_each version is 10 times faster than the raw loop. What?</p>
<a id="more"></a>
<h1 id="Test-for-yourself"><a href="#Test-for-yourself" class="headerlink" title="Test for yourself"></a>Test for yourself</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;array&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;chrono&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">array</span>&lt;<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;, 6&gt; buffers;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; ++i) &#123;</div><div class="line">		buffers[i] = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;(<span class="number">480000</span>, <span class="number">0.5f</span>);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">auto</span> tstart = <span class="built_in">std</span>::chrono::high_resolution_clock::now();</div><div class="line">	<span class="keyword">auto</span> accum = <span class="number">0</span>;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; ++i) &#123;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; buffers[i].size(); ++j) &#123;</div><div class="line">			<span class="keyword">if</span> (buffers[i][j] &lt; <span class="number">1.0f</span>)</div><div class="line">				++accum;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">auto</span> tend = <span class="built_in">std</span>::chrono::high_resolution_clock::now();</div><div class="line">	<span class="keyword">auto</span> duration = tend - tstart;</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Raw loop: "</span> &lt;&lt; <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::milliseconds&gt;(duration).count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line"></div><div class="line">	tstart = <span class="built_in">std</span>::chrono::high_resolution_clock::now();</div><div class="line">	accum = <span class="number">0</span>;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; buffer : buffers) &#123;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; value : buffer) &#123;</div><div class="line">			<span class="keyword">if</span> (value &lt; <span class="number">1.0f</span>)</div><div class="line">				++accum;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	tend = <span class="built_in">std</span>::chrono::high_resolution_clock::now();</div><div class="line">	duration = tend - tstart;</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt;<span class="string">"Range-based for loop: "</span> &lt;&lt; <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::milliseconds&gt;(duration).count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line"></div><div class="line">	tstart = <span class="built_in">std</span>::chrono::high_resolution_clock::now();</div><div class="line">	accum = <span class="number">0</span>;</div><div class="line">	<span class="built_in">std</span>::for_each(buffers.begin(), buffers.end(),</div><div class="line">		[&amp;accum](<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&amp; buffer) &#123;</div><div class="line">			<span class="built_in">std</span>::for_each(buffer.begin(), buffer.end(), [&amp;accum](<span class="keyword">float</span> value) &#123; <span class="keyword">if</span> (value &lt; <span class="number">1.0f</span>) ++accum; &#125;);</div><div class="line">		&#125;</div><div class="line">	);</div><div class="line">	tend = <span class="built_in">std</span>::chrono::high_resolution_clock::now();</div><div class="line">	duration = tend - tstart;</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"std::for_each: "</span> &lt;&lt; <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::milliseconds&gt;(duration).count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>I was using VS2015 under Debug build. Here’s the output:</p>
<blockquote>
<p>Raw loop: 978</p>
<p>Range-based for loop: 426</p>
<p>std::for_each: 66</p>
</blockquote>
<p>However when I switched to Release build:</p>
<blockquote>
<p>Raw loop: 2</p>
<p>Range-based for loop: 2</p>
<p>std::for_each: 5</p>
</blockquote>
<p>That’s what I’ve been expecting. And when I changed time precision to nanoseconds it turned out that raw loop is slightly faster than the range-based for loop.</p>
<h1 id="Lesson-learned"><a href="#Lesson-learned" class="headerlink" title="Lesson learned"></a>Lesson learned</h1><p>The compiler sure knows how to optimize your code. So do your profiling with optimization on.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Today I was testing the performance of a piece of code, which is basically accessing each element in a container within a for loop. But the result is quite shocking because I found the std::for_each version is 10 times faster than the raw loop. What?&lt;/p&gt;
    
    </summary>
    
      <category term="C++" scheme="https://unclejimbo.github.io/categories/C/"/>
    
    
      <category term="C++" scheme="https://unclejimbo.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>RTTI in C++</title>
    <link href="https://unclejimbo.github.io/2017/02/22/RTTI-in-C++/"/>
    <id>https://unclejimbo.github.io/2017/02/22/RTTI-in-C++/</id>
    <published>2017-02-22T18:30:37.000Z</published>
    <updated>2017-02-22T13:17:01.077Z</updated>
    
    <content type="html"><![CDATA[<p>RunTime Type Information or RunTime Type Identification, or just RTTI, is a useful feature in C++ language. As its name suggests, this facility gives you the ability to query type information at runtime.</p>
<ul>
<li><code>dynamic_cast&lt;&gt;</code></li>
<li><code>typeid()</code></li>
</ul>
<p>Those are the main tools to achieve RTTI. Some of you may frown upon this RTTI thing because it seems to have a bad reputation out there, and there is a good reason for that. Generally you want to stay away from this feature because static typing is much safer than dynamic typing thanks to the compiler, and it gives you runtime overheads as well. So why is it useful?</p>
<a id="more"></a>
<h1 id="Walking-around-the-Inheritance-Graph"><a href="#Walking-around-the-Inheritance-Graph" class="headerlink" title="Walking around the Inheritance Graph"></a>Walking around the Inheritance Graph</h1><p>Generally we use virtual inheritance and polymorphism because we only care about the interface but not the exact implementations. However, some derived classes may have specific method that doesn’t fit into the interface in any way. Although it is possible to add another virtual function to the base class and all its derived classes and let them provide empty implementations. But doesn’t that sounds awkward when you tell every Person class to program() when a Programmer class has to? It might be more reasonable to recover the true type (not exactly right with dynamic_cast&lt;&gt;, I’ll talk about it later) of a polymorphic type and call the class’s unique function as needed.</p>
<p><code>dynamic_cast&lt;&gt;</code> is your first choice to achieve this. Consider the following codes,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">class Base1</div><div class="line">&#123;</div><div class="line">  virtual void base1Fcn();</div><div class="line">&#125;</div><div class="line"></div><div class="line">class Base2</div><div class="line">&#123;</div><div class="line">  virtual void base2Fcn();</div><div class="line">&#125;</div><div class="line"></div><div class="line">class Derived : public Base1, public Base2</div><div class="line">&#123;</div><div class="line">  void base1Fcn() override;</div><div class="line">  void base2Fcn() override;</div><div class="line">  virtual void derivedFcn();</div><div class="line">&#125;</div><div class="line"></div><div class="line">// upcast, which is just good old polymorphic pointers</div><div class="line">Derived* pd = new Derived();</div><div class="line">Base1* pb1 = pd;</div><div class="line">Base2* pb2 = dynamic_cast&lt;Base2*&gt;(pd); // Same as above, no explicit cast needed</div><div class="line"></div><div class="line">// downcast, which casts base pointer to derived class</div><div class="line">Base1* pb1 = new Derived();</div><div class="line">Derived* pd = dynamic_cast&lt;Derived*&gt;(pb1);</div><div class="line"></div><div class="line">// crosscast, which casts from one base pointer to a sibling</div><div class="line">Base1* pb1 = new Derived();</div><div class="line">Base2* pb2 = dynamic_cast&lt;Base2*&gt;(pb1);</div></pre></td></tr></table></figure>
<p>As is quite clear in the codes and comments, you can use <code>dynamic_cast&lt;&gt;</code> to walk around the inheritance graph get to the specific class you need, as long as virtual inheritance is used. The name upcast, downcast, and crosscast come from the tradition to draw base class on top of the derived in an inheritance graph.</p>
<h1 id="Querying-the-Exact-Type"><a href="#Querying-the-Exact-Type" class="headerlink" title="Querying the Exact Type"></a>Querying the Exact Type</h1><p>Remember earlier I said that <code>dynamic_cast&lt;&gt;</code> isn’t really about recovering the true type of a polymorphic pointer. Let’s say we have another class down the inheritance graph,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">class FurtherDerived : public Derived</div><div class="line">&#123;</div><div class="line">  void derivedFcn() override;</div><div class="line">&#125;</div><div class="line"></div><div class="line">Base1* pb1 = new FurtherDerived();</div><div class="line">Derived* pd = dynamic_cast&lt;Derived*&gt;(pb1); // See? Derived is not the real type pb1 points to</div><div class="line">pd-&gt;derivedFc(); // But this will work</div></pre></td></tr></table></figure>
<p>Bjarne Stroustrup has made this quite clear in TCPL,</p>
<blockquote>
<p>From a design perspective, dynamic_cast can be seen as a mechanism for asking an object if it provides a given interface</p>
</blockquote>
<p>As long as the class provides the requested interface, <code>dynamic_cast&lt;&gt;</code> will work. In other words it works for all derived classes down the road. So how do we find out the exact type of an object? It’s <code>typeid()</code> to the rescue.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">typeid(*pb1) == typeid(FurtherDerived); // true</div><div class="line">typeid(*pb1) == typeid(Derived); // false</div></pre></td></tr></table></figure>
<p>The return type of <code>typeid()</code> is <code>std::type_info</code> which has some other useful functions like hashing and name string. Keep in mind that <code>typid()</code> and <code>dynamic_cast&lt;&gt;</code> are designed for different tasks so you shouldn’t misuse both.</p>
<h1 id="Alternatives"><a href="#Alternatives" class="headerlink" title="Alternatives"></a>Alternatives</h1><p>You could almost always redesign the inheritance relationships and use virtual functions to do dynamic dispatching. But do know that you have the right tools at hands and don’t hesitate to use them when needed.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RunTime Type Information or RunTime Type Identification, or just RTTI, is a useful feature in C++ language. As its name suggests, this facility gives you the ability to query type information at runtime.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dynamic_cast&amp;lt;&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;typeid()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those are the main tools to achieve RTTI. Some of you may frown upon this RTTI thing because it seems to have a bad reputation out there, and there is a good reason for that. Generally you want to stay away from this feature because static typing is much safer than dynamic typing thanks to the compiler, and it gives you runtime overheads as well. So why is it useful?&lt;/p&gt;
    
    </summary>
    
      <category term="C++" scheme="https://unclejimbo.github.io/categories/C/"/>
    
    
      <category term="C++" scheme="https://unclejimbo.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Implementing MVCNN with Caffe, Part I</title>
    <link href="https://unclejimbo.github.io/2017/01/05/2017-1-5-Implementing-MVCNN-with-Caffe-Part-I/"/>
    <id>https://unclejimbo.github.io/2017/01/05/2017-1-5-Implementing-MVCNN-with-Caffe-Part-I/</id>
    <published>2017-01-05T00:00:00.000Z</published>
    <updated>2017-01-05T08:35:19.601Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This post is about a paper published in ICCV2015, called <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf" target="_blank" rel="external">“Multi-view Convolutional Neural Networks for 3D Shape Recognition”</a>. It describes a method to classify 3d shape models using 2d image classification networks. While the authors have open-sourced their matlab implementation on <a href="https://github.com/suhangpro/mvcnn" target="_blank" rel="external">GitHub</a>, here I’ll try to implement this network with Caffe.</p>
<p>In the first half of this two-part blog, I’ll quickly explain the core idea of this paper. After that I’ll try to implement a naive version of this network. While in part II I’ll go through the details on implementing MVCNN as the paper describes.</p>
<p>The complete codes and scripts in this blog can be found at my <a href="https://github.com/unclejimbo/mvcnn-caffe" target="_blank" rel="external">GitHub repo</a>.</p>
<p><img src="https://camo.githubusercontent.com/f505454fa4d971db8b85b35ad7cac63795d3eaa0/687474703a2f2f7669732d7777772e63732e756d6173732e6564752f6d76636e6e2f696d616765732f6d76636e6e2e706e67" alt=""><br>(<em>The network architecture of MVCNN</em>)</p>
<a id="more"></a>
<h1 id="The-Power-of-Multi-View"><a href="#The-Power-of-Multi-View" class="headerlink" title="The Power of Multi-View"></a>The Power of Multi-View</h1><p>Traditional 3d shape recognition algorithms are generally based on heuristic descriptors such as Spherical Harmonics. More recent advances like ShapeNets tried to voxelize the model and train a deep neural network. On the other hand, MVCNN tries to leverage the power of image classification CNNs, because public image datasets such as ImageNet is much larger than 3d model datasets and state-of-the-art networks on ilsvrc have achieved pretty high precision on classification tasks.</p>
<p>So how about rendering a 3d shape model under different viewpoints and training a 2d CNN with rendered images? Then you can input the rendered views of an unknown model and try to decide its category. This is exactly what we’ll implement in this post. We’ll use multiple rendered images as input and simply do a majority vote to decide the final label for the model.</p>
<p>However, the MVCNN is a bit more complicated in the way it combines multi-view representations. The authors train each view with a different network and introduce a view-pooling layer to combine multiple networks into one, as can be seen on the figure above. View-pooling at its core is simply max-pooling, extracting the largest value at each pixel among all views. More on this topic next time, but let’s first go ahead and implement the simple one :).</p>
<h1 id="Training-MVCNN-Directly-without-View-Pooling"><a href="#Training-MVCNN-Directly-without-View-Pooling" class="headerlink" title="Training MVCNN Directly without View Pooling"></a>Training MVCNN Directly without View Pooling</h1><h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><ul>
<li>Caffe :white_check_mark:</li>
<li>Python2.7(Anaconda2) :white_check_mark:</li>
<li>Scikit-Image :white_check_mark:</li>
</ul>
<h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><p>You can download the rendered images in their <a href="https://github.com/suhangpro/mvcnn" target="_blank" rel="external">repository</a>. Here I’ll use the modelnet40v2 dataset. On the other hand, you can directly download the models from <a href="http://modelnet.cs.princeton.edu/#" target="_blank" rel="external">Princeton ModelNet</a> and render by yourself if you want better rendering qualities. Although the authors have argued that rendering method is irrelevant with classification precisions.</p>
<p>Please follow the steps described <a href="https://github.com/unclejimbo/mvcnn-caffe/tree/master/modelnet40v2" target="_blank" rel="external">here</a> to preprocess the dataset. Basically all it does is to first pad all images into 256x256, and split a validation set out of training set with ratio 1:9. Then we prepare label text files and compile images into leveldb files to feed into the network. You can sample the inputs if you think the full dataset is too large. I use leveldb rather than lmdb because lmdb seems to have bug with large amount data. If you’re not familiar with this procedure, please check out this <a href="http://caffe.berkeleyvision.org/gathered/examples/imagenet.html" target="_blank" rel="external">tutorial</a>.</p>
<h2 id="Network-Setup"><a href="#Network-Setup" class="headerlink" title="Network Setup"></a>Network Setup</h2><p>Here we’ll try to fine-tune the bvlc-reference-caffenet that caffe provides. The method is pretty much the same as the official <a href="http://caffe.berkeleyvision.org/gathered/examples/finetune_flickr_style.html" target="_blank" rel="external">tutorial on fine-tuning flickr style data</a>. There are a few things to note. First, because we are using the 40 class version of modelnet so the output of this network should be 40. And I’ve fixed the learning rate for all conv layers. Moreover, the latest fc layer’s learning rate is set 5 times higher. Check the prototxt in my <a href="https://github.com/unclejimbo/mvcnn-caffe" target="_blank" rel="external">repo</a> for details.</p>
<h2 id="Go-Ahead-and-Training"><a href="#Go-Ahead-and-Training" class="headerlink" title="Go Ahead and Training"></a>Go Ahead and Training</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">caffe train -solver ./mvcnn_caffenet_simple/solver.prototxt -weights your_caffe_root/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel -gpu 0</div></pre></td></tr></table></figure>
<p>I’ve tried several parameter setups and the best could give around 72.5% test accuracy. Go play with the parameters by yourself.</p>
<h2 id="Use-Majority-Vote-to-Classify-a-3D-Model"><a href="#Use-Majority-Vote-to-Classify-a-3D-Model" class="headerlink" title="Use Majority Vote to Classify a 3D Model"></a>Use Majority Vote to Classify a 3D Model</h2><p>We’ll now use 80 views as input and take the majority of the predictions as label for this model. The file <a href="https://github.com/unclejimbo/mvcnn-caffe/blob/master/classify_model_simple.py" target="_blank" rel="external">classify_model_simple.py</a> contains the source code for this method:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">import caffe</div><div class="line">import os</div><div class="line">import sys</div><div class="line">import re</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">n_views = 80 # change this if you are not using 80 views per model&quot;</div><div class="line">n_classes = 40 # change this if you are not using 40 class dataset</div><div class="line">model = &quot;./mvcnn_caffenet_simple/deploy.prototxt&quot; # change this to your model defiintion</div><div class="line">weights = &quot;./mvcnn_caffenet_simple/caffenet_train_iter_15000.caffemodel&quot; # change this to your trained model</div><div class="line">mean_file = &quot;./modelnet40v2/mean.binaryproto&quot; # change this to your mean file</div><div class="line">label_name_file = &quot;./modelnet40v2/label_name.txt&quot; # change this to your label_name file</div><div class="line"></div><div class="line">def predict(images, net, transformer):</div><div class="line">    if (len(images) != n_views):</div><div class="line">        sys.exit(&quot;Error: expecting &quot;, n_views, &quot; images in a batch&quot;)</div><div class="line"></div><div class="line">    votes = np.zeros(n_views)</div><div class="line">    for img in images:</div><div class="line">        net.blobs[&apos;data&apos;].data[...] = transformer.preprocess(&apos;data&apos;, img)</div><div class="line">        net.forward()</div><div class="line">        scores = net.blobs[&apos;prob&apos;].data[0].flatten()</div><div class="line">        prediction = np.argmax(scores)</div><div class="line">        votes[prediction] += 1</div><div class="line">    return np.argmax(votes)</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    if (len(sys.argv) != 2):</div><div class="line">        sys.exit(&quot;Usage: python classify_model_simple.py /path/to/image_directory/&quot;)</div><div class="line"></div><div class="line">    files = os.listdir(sys.argv[1])</div><div class="line">    if (len(files) % n_views != 0):</div><div class="line">        sys.exit(&quot;Error: num of input images should be divisible by &quot; + str(n_views))</div><div class="line"></div><div class="line">    # Import label-name correspondence</div><div class="line">    label_name = &#123;&#125;</div><div class="line">    with open(label_name_file) as f:</div><div class="line">        for line in f:</div><div class="line">            (label, name) = line.split()</div><div class="line">            label_name[int(label)] = name</div><div class="line"></div><div class="line">    # Net</div><div class="line">    net = caffe.Net(model, weights, caffe.TEST)</div><div class="line"></div><div class="line">    # Convert image mean</div><div class="line">    mean_blob = caffe.proto.caffe_pb2.BlobProto()</div><div class="line">    mean_bin = open(mean_file, &apos;rb&apos;).read()</div><div class="line">    mean_blob.ParseFromString(mean_bin)</div><div class="line">    mean = np.array(caffe.io.blobproto_to_array(mean_blob))</div><div class="line">    # have to manually slice mean size because caffe assume mean size to be the same as input dims,</div><div class="line">    # during training both mean and image sizes could automatically get cropped to network input dims,</div><div class="line">    # however when testing only image size could be cropped automatically by deployment input layer</div><div class="line">    mean = mean[0, :, 14:-15, 14:-15]</div><div class="line"></div><div class="line">    # Transform data</div><div class="line">    transformer = caffe.io.Transformer(&#123;&apos;data&apos;: net.blobs[&apos;data&apos;].data.shape&#125;)</div><div class="line">    transformer.set_transpose(&apos;data&apos;, (2, 0, 1))</div><div class="line">    transformer.set_mean(&apos;data&apos;, mean)</div><div class="line">    transformer.set_raw_scale(&apos;data&apos;, 255)</div><div class="line">    transformer.set_channel_swap(&apos;data&apos;, (2, 1, 0))</div><div class="line"></div><div class="line">    # Load images and predict</div><div class="line">    count = 0;</div><div class="line">    right = 0;</div><div class="line">    for i in range(len(files) / n_views):</div><div class="line">        images = []</div><div class="line">        for j in range(i*n_views, (i+1)*n_views):</div><div class="line">            img = caffe.io.load_image(os.path.join(sys.argv[1], files[j]))</div><div class="line">            images.append(img)</div><div class="line"></div><div class="line">        # Extract label from file name</div><div class="line">        pos = re.search(&quot;\d&quot;, files[j])</div><div class="line">        name = files[j][:pos.start()-1]</div><div class="line">        pred_label = predict(images, net, transformer)</div><div class="line">        pred_name = label_name[int(pred_label)]</div><div class="line">        print(&quot;label: &quot; + name + &quot;, &quot; + &quot;prediction: &quot; + pred_name)</div><div class="line">        count += 1</div><div class="line">        if name == pred_name:</div><div class="line">            right += 1</div><div class="line"></div><div class="line">    print(&quot;Accuracy: &quot; + str(right / float(count)))</div></pre></td></tr></table></figure></p>
<p>The output accuracy is about 78.3%, which is higher than one image classification output.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Great, this simple network does what we’ve expected, although the result is much more inferior to the result achieved in the paper. But this isn’t a surprise because it is image loss rather than model loss that is minimized during training. This also explains the high training error because some views are quite different from others so this adds a lot of noise in our data.</p>
<p>I’ll implement the full mvcnn with view-pooling in the next post, and see if it works better. Stay tuned.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>Su H, Maji S, Kalogerakis E, et al. Multi-view convolutional neural networks for 3d shape recognition[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 945-953.</p>
<p><a href="http://seiya-kumada.blogspot.jp/2015/10/3d-object-recognition-by-caffe.html" target="_blank" rel="external">http://seiya-kumada.blogspot.jp/2015/10/3d-object-recognition-by-caffe.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;This post is about a paper published in ICCV2015, called &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf&quot;&gt;“Multi-view Convolutional Neural Networks for 3D Shape Recognition”&lt;/a&gt;. It describes a method to classify 3d shape models using 2d image classification networks. While the authors have open-sourced their matlab implementation on &lt;a href=&quot;https://github.com/suhangpro/mvcnn&quot;&gt;GitHub&lt;/a&gt;, here I’ll try to implement this network with Caffe.&lt;/p&gt;
&lt;p&gt;In the first half of this two-part blog, I’ll quickly explain the core idea of this paper. After that I’ll try to implement a naive version of this network. While in part II I’ll go through the details on implementing MVCNN as the paper describes.&lt;/p&gt;
&lt;p&gt;The complete codes and scripts in this blog can be found at my &lt;a href=&quot;https://github.com/unclejimbo/mvcnn-caffe&quot;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://camo.githubusercontent.com/f505454fa4d971db8b85b35ad7cac63795d3eaa0/687474703a2f2f7669732d7777772e63732e756d6173732e6564752f6d76636e6e2f696d616765732f6d76636e6e2e706e67&quot; alt=&quot;&quot;&gt;&lt;br&gt;(&lt;em&gt;The network architecture of MVCNN&lt;/em&gt;)&lt;/p&gt;
    
    </summary>
    
      <category term="Computer Vision" scheme="https://unclejimbo.github.io/categories/Computer-Vision/"/>
    
    
      <category term="Computer Graphics" scheme="https://unclejimbo.github.io/tags/Computer-Graphics/"/>
    
      <category term="Computer Vision" scheme="https://unclejimbo.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="https://unclejimbo.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Upright Orientation Detection Methods</title>
    <link href="https://unclejimbo.github.io/2016/12/15/Uprigt-Summary/"/>
    <id>https://unclejimbo.github.io/2016/12/15/Uprigt-Summary/</id>
    <published>2016-12-15T14:41:00.000Z</published>
    <updated>2016-12-28T08:46:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Detecting the upright orientation of a 3d shape is a basic yet difficult problem in CG literature. It often serves as a preprocessing step for many geometry processing and shape analysis algorithms such as shape recognition and retrieval.</p>
<p><img src="http://7xllm5.com1.z0.glb.clouddn.com/upright.png" alt="Upright orientation of a chair"></p>
<p>(<em>Upright orientation of a chair</em>)</p>
<a id="more"></a>
<p>For many shapes, there exist natural bases where they naturally stand on so the upright direction should be the normal to that base. However, other shapes (consider a swimmer) may not exhibit a clear standing base so telling this problem could be more subtle.</p>
<p>The following sections summarize existing methods (to the best of my knowledge, they are all of them) and address their issues and limitations.</p>
<h1 id="Known-Methods"><a href="#Known-Methods" class="headerlink" title="Known Methods"></a>Known Methods</h1><ol>
<li>Fu08<ul>
<li>Pipeline:<ul>
<li>computing convex hull</li>
<li>generating hand-crafted features</li>
<li>training on random forest + support vector machine</li>
</ul>
</li>
<li>Data Set: Princeton Shape Benchmark</li>
<li>Precision: 87.5%</li>
<li>Timing: 10.62s (2.13GHz)</li>
</ul>
</li>
<li>Lin12<ul>
<li>Pipeline:<ul>
<li>computing convex hull</li>
<li>generating hand-crafted features</li>
<li>combining scores linearly</li>
</ul>
</li>
<li>Data Set: Princeton Shape Benchmark (120 selected)</li>
<li>Precision: 79%</li>
<li>Timing: 154s (2.33GHz <em>including time to compute best view</em>)</li>
</ul>
</li>
<li>Jin12 &amp; Wang14 (<em>the second method is an improvement on the first one, statistics below accounts for the newer method only</em>)<ul>
<li>Pipeline:<ul>
<li>optimizing against the projection/tensor matrix’s rank</li>
<li>low rank matrix corresponds to a model posed in axis-aligned manner</li>
<li>finding the base from 6 candidate orientations</li>
</ul>
</li>
<li>Data Set: Princeton Shape Benchmark</li>
<li>Precision: about 70%</li>
<li>Timing: 1-2mins (3.30GHz)</li>
</ul>
</li>
<li>Han15<ul>
<li>Pipeline:<ul>
<li>using view selection methods to compute view scores</li>
<li>clustering views whose scores are low as model base candidates</li>
<li>determining upright orientation w.r.t. several hand-crafted features</li>
</ul>
</li>
<li>Data Set: Princeton Shape Benchmark</li>
<li>Precision: 79%</li>
<li>Timing: 7.8s (2.0GHz)</li>
</ul>
</li>
<li>Liu16<ul>
<li>Pipeline:<ul>
<li>voxelizing shapes</li>
<li>training a classification convnet to dispatch input shapes</li>
<li>training a regression convnet for each shape category to find upright orientation</li>
</ul>
</li>
<li>Data Set: ModelNet10</li>
<li>Precision: 90.1% (<em>with convnets clustring</em>)</li>
<li>Timing: 0.15s (3.20GHz)</li>
</ul>
</li>
</ol>
<h1 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h1><ol>
<li>Most man-made models are supposed to stand on its own so computing the convex hull and assigning a hull face as supporting base is appropriate. On the other hand, natural models like human may pose large deformation so there is no clear supporting base. Previous methods failed to solve this conflict.</li>
<li>Method 1 and 5 are still short on training data and the generation power on unseen models is limited.</li>
<li>Other methods based on hand-crafted features and criterions usually give inferior results.</li>
</ol>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>Fu H, Cohen-Or D, Dror G, et al. Upright orientation of man-made objects[C]. ACM transactions on graphics (TOG). ACM, 2008, 27(3): 42.</p>
<p>Lin C K, Tai W K. Automatic upright orientation and good view recognition for 3D man-made models[J]. Pattern Recognition, 2012, 45(4): 1524-1530.</p>
<p>Jin Y, Wu Q, Liu L. Unsupervised upright orientation of man-made models[J]. Graphical Models, 2012, 74(4): 99-108.</p>
<p>Wang W, Liu X, Liu L. Upright orientation of 3D shapes via tensor rank minimization[J]. Journal of Mechanical Science and Technology, 2014, 28(7): 2469-2477.</p>
<p>Han HL,Wang WC,Hua M.Getting upright orientation of 3D objects via viewpoint scoring.Ruan Jian Xue Bao/Journal of Software,2015,26(10):2720?2732(in Chinese).</p>
<p>Liu Z, Zhang J, Liu L. Upright orientation of 3D shapes with Convolutional Networks[J]. Graphical Models, 2016, 85: 22-29.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;Detecting the upright orientation of a 3d shape is a basic yet difficult problem in CG literature. It often serves as a preprocessing step for many geometry processing and shape analysis algorithms such as shape recognition and retrieval.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xllm5.com1.z0.glb.clouddn.com/upright.png&quot; alt=&quot;Upright orientation of a chair&quot;&gt;&lt;/p&gt;
&lt;p&gt;(&lt;em&gt;Upright orientation of a chair&lt;/em&gt;)&lt;/p&gt;
    
    </summary>
    
      <category term="Computer Graphics" scheme="https://unclejimbo.github.io/categories/Computer-Graphics/"/>
    
    
      <category term="Computer Graphics" scheme="https://unclejimbo.github.io/tags/Computer-Graphics/"/>
    
      <category term="Research Log" scheme="https://unclejimbo.github.io/tags/Research-Log/"/>
    
  </entry>
  
</feed>
